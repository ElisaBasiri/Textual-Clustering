# -*- coding: utf-8 -*-
"""DataMining Project3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VMS0PAL7Zh8M7jj4mA1SxsAppGs1uTj8

<font face="B Mitra" size=4>
<div dir=rtl align=center>
<br>
<img src="https://aut.ac.ir/templates/tmpl_modern01/images/logo_fa.png" alt="Amirkabir University Logo" width="100">
<br>
<font size=6>
<b>پروژه سوم داده کاوی</b>
<br>
<b><font size=5> استاد درس: دکتر فاطمه شاکری</b>
<hr>
</div>
</font>

<font face="B Mitra">
<div dir=rtl>
<font size=5>
کتابخانه های موردنیاز را در این بخش بارگذاری کنید.
</p>
</font>
</div>
</font>
"""

!pip install langdetect
!pip install scispacy
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_lg-0.5.4.tar.gz
!pip install spacy
!pip install pandas
!pip install sklearn
!pip install matplotlib
!pip install seaborn
!pip install numpy
!pip install nltk

"""<font face="B Mitra">
<div dir=rtl>
<font size=5>
نصب پکیج <code>spacy</code> ممکن است وابسته به سیستم‌عامل و پکیج‌منیجر شما نیاز به دستوری متفاوت داشته‌باشد. دستور مناسب را می‌تواند با استفاده از
<a href="https://spacy.io/usage">این لینک</a> بیابید.
</p>
</font>
</div>
</font>
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries
import numpy as np
import pandas as pd

# Plotting and visualization
import matplotlib.pyplot as plt
plt.style.use('ggplot')
# %config InlineBackend.figure_formats = ['svg']
import seaborn as sns

# Used to draw a progress bar for longer method calls
from tqdm import tqdm
tqdm.pandas()

# Used to detect language used in each document
from langdetect import detect
from langdetect import DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
DetectorFactory.seed = 0

# Pre-trained natural language processing pipeline for biomedical use
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import en_core_sci_lg

# Stemming libraries
import nltk
from nltk.stem import PorterStemmer
# Ensure nltk stemmer is downloaded
nltk.download('punkt')

# Used to import list of punctuations
import string

# Feature extraction (text vectorizers)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Dimensionality reduction
from sklearn.decomposition import PCA
from sklearn.decomposition import NMF
from sklearn.manifold import TSNE

# Clustering and evaluation
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import cdist

# Topic modeling
from sklearn.decomposition import LatentDirichletAllocation

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> لود کردن مجموعه داده </b>
<br>
<p align="justify">
<font size=5>
فایل مجموعه داده در <a href="https://drive.google.com/file/d/15E8FLX0C-6qpK-lDBEQJXw00Lsdcvjae/view?usp=sharing">گوگل درایو</a> در اختیار شما قرار گرفته است:
<br>
همچنین در صورت استفاده از گوگل کولب با استفاده از دستور زیر میتوانید مجموعه داده را از گوگل درایو در نوتبوک خود دانلود کنید.
</p>
</font>
</div>
</font>

!gdown 15E8FLX0C-6qpK-lDBEQJXw00Lsdcvjae

<font face="B Mitra">
<div dir=rtl>
<font size=5>
در شروع کار،دیتافریم موردنظر خود را ایجاد کردیم و مقادیر خالی را با space جایگزین کردیم.
دلیل این کار جلوگیری از خطا در مصورسازی انتهایی است.<br>
سپس با توجه به زمانبر بودن اجرا،حداقل یک سمپل 1500 تایی نمونه برداری کردیم.
<br>
برای کسب نتایج بهتر و مصورسازی مناسبتر میتوانیم تعداد سمپل را افزایش دهیم.
</p>
</font>
</div>
"""

!gdown 15E8FLX0C-6qpK-lDBEQJXw00Lsdcvjae
df_10k = pd.read_csv('10k_df.csv')
df_10k.fillna(value=" ",inplace=True)
df = df_10k.sample(1500, random_state=42)
del df_10k
df.head()

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> پیش پردازش متن</b>
<br>
<p align="justify">
<font size=5>
 برای پیش پردازش متون از کتابخانه های مختلفی از جمله NLTK ،Gensim یا Spacy میتوان استفاده کرد.
 <br>
 مواردی که در پیش پردازش متون باید به آن توجه کرد.
 <ol>
 <br>
 <li> <b>تشخیص زبان مورداستفاده مقاله ها:</b>
  در این مجموعه داده میتوانید  با استفاده از کتابخانه langdetect زبان مورد استفاده مقاله را در ویژگی <code>body_text</code> بررسی کنید.
 با توجه به نتیجه نهایی خواهید دید که زبان انگلیسی زبان غالب در این مقاله هاست بنابراین میتوان مقاله های غیر انگلیسی را حذف کرد.
 </li>
 <li> <b>توکنایز کردن (Tokenize) مدل ها:</b>
 در این مجموعه داده از  پایپ لاین  <code>en_core_sci_lg</code> از کتابخانه  spaCy به دلیلی پشتبانی از داده های زیستی و پزشکی و با بردارهای 600 هزار کلمه ای و واژگان بزرگتر میتوانیم استفاده کنیم.<br>
 به دلیل اینکه SpaCy حداکثر یک میلیون کاراکتر را بررسی میکند میتوان با استفاده از max_length این محدودیت را کاهش داد:
 لطفا از parser مشخص شده در بخش زیر برای توکنایز کردن متن خود استفاده کنید.


 </li>
 <li> <b> Lemmatization یا Stemming (ریشه یابی) </b>
 </li>
 <li> <b>حذف Stopwords</b> </li>
 <li> <b>حذف علائم نشانه گذاری</b> </li>
 </ol>
<br>
درمورد هر یک از موارد بالا جستجو کنید و پیش پردازش های لازم را برای ویژگی "body text" مقاله انجام دهید.
</p>
</font>
</div>
</font>

#2)
تابع en_core_sci_lg یک مدل spaCy از پیش آموزش دیده را بارگیری می کند که به طور
خاص برای پردازش متون علمی به زبان انگلیسی طراحی شده است. این چیزی است که انجام می دهد:


1.   مدل را بارگذاری می کند: این تابع مدل spaCy از پیش آموزش دیده با نام en_core_sci_lg را بارگیری می کند.
2.   بهینه سازی شده برای متن علمی: این مدل برای پردازش متون علمی بهینه شده است، که ممکن است دارای واژگان تخصصی، اصطلاحات فنی و الگوهای نحوی منحصر به فردی باشد که معمولاً در مقالات دانشگاهی، مقالات تحقیقاتی و سایر اسناد علمی یافت می شود.
3.Tokenization: عملکرد اصلی این مدل توکن کردن متن ورودی است، به این معنی که آن را به کلمات جداگانه، علائم نقطه گذاری و دیگر واحدهای معنی دار به نام نشانه ها تقسیم می کند.
4.برچسب‌گذاری قسمتی از گفتار (Tagger): به‌طور پیش‌فرض، مدل‌های spaCy برچسب‌گذاری بخشی از گفتار را انجام می‌دهند، که دسته‌های دستوری (مانند اسم، فعل، صفت) را به هر نشانه اختصاص می‌دهد. با این حال، در قطعه کد ما، مؤلفه برچسب صراحتاً غیرفعال است، احتمالاً سرعت پردازش را بهبود می‌بخشد زیرا ممکن است برچسب‌گذاری بخشی از گفتار برای کارهای خاصی ضروری نباشد.
5.شناسایی نهاد نامگذاری شده (NER): به طور مشابه، مؤلفه تشخیص نهاد نامگذاری شده (NER) غیرفعال است. NER موجودیت های نامگذاری شده مانند نام افراد، سازمان ها، مکان ها و غیره را در متن شناسایی می کند. غیرفعال کردن آن همچنین می تواند سرعت پردازش را بهبود بخشد، به خصوص اگر تشخیص موجودیت نامگذاری شده برای کار شما لازم نباشد.

# 3)A.
Lemmatization فرآیند کاهش کلمات به شکل پایه یا ریشه آنها است که لم نامیده می شود، در حالی که هنوز اطمینان حاصل می شود که شکل کاهش یافته به واژگان زبان تعلق دارد. این به عادی سازی کلمات کمک می کند به طوری که تغییرات یک کلمه به عنوان همان کلمه در نظر گرفته می شود.
---
در زمینه داده های متنی:
*   مثال:

به عنوان مثال، کلمات "running"، "run" و "runner" همگی به شکل پایه "run" تبدیل می شوند.
*   پیش پردازش متن:

Lemmatization معمولاً به عنوان یک مرحله پیش پردازش در وظایف پردازش زبان طبیعی (NLP) مانند طبقه بندی متن، تجزیه و تحلیل احساسات و مدل سازی موضوع استفاده می شود.
این به کاهش ابعاد داده‌های متنی با تبدیل اشکال مختلف کلمات به شکل پایه مشترک کمک می‌کند.
*   فواید:

Lemmatization می تواند به بهبود دقت و اثربخشی مدل های NLP با اطمینان از اینکه کلمات با معانی مشابه به عنوان یک کلمه در نظر گرفته می شوند، کمک کند.
همچنین می‌تواند به کاهش پراکندگی داده‌های متنی کمک کند، به‌ویژه زمانی که با واژگان بزرگ سروکار دارید.
*   پیاده سازی:

کتابخانه‌هایی مانند spaCy، NLTK، و TextBlob قابلیت‌های کلمه‌سازی را ارائه می‌کنند.
در spaCy، lemmatization به عنوان بخشی از فرآیند توکن سازی به طور پیش فرض انجام می شود. شما می توانید با استفاده از ویژگی .lemma_ توکن های spaCy به شکل لماتیزه شده توکن ها دسترسی داشته باشید.
در NLTK می توانید از کلاس WordNetLemmatizer برای انجام واژه سازی استفاده کنید.
در TextBlob، متد lemmatize() برای lemmatization در دسترس است.

#3)B.
stemming فرآیند کاهش کلمات به شکل ریشه یا پایه آنها با حذف پسوندها و پیشوندها است. شکل ریشه ای حاصل ممکن است یک کلمه معتبر در زبان نباشد، اما معنای اصلی کلمه را نشان می دهد. Stemming تکنیکی است که در پردازش زبان طبیعی (NLP) و بازیابی اطلاعات برای ساده‌سازی واژگان و بهبود کارایی پردازش متن استفاده می‌شود.
در قطعه کدی که ارائه شده  stemming به عنوان بخشی از خط لوله پیش پردازش متن در preprocess_textfunction اعمال می شود. این چیزی است که انجام می دهد:



1.   Stemmer Initialization:


*   کد یک الگوریتم پایه را با استفاده از Porter Stemmer از کتابخانه NLTK مقداردهی اولیه می کند.
*   Porter Stemmer یک الگوریتم پایه پرکاربرد است که کلمات را بر اساس قوانین ساده به شکل اصلی خود کاهش می دهد.


2.   تابع متن پیش پردازش:


*   این تابع یک ورودی متن را می گیرد و آن را پیش پردازش می کند.

*   با استفاده از مقداری تجزیه کننده (تجزیه کننده) متن را نشانه گذاری می کند و متن را به حروف کوچک تبدیل می کند.
*   سپس کلمات توقف (کلمات رایج مانند "the"، "is"، "and") و علائم نگارشی را از متن نشانه گذاری شده حذف می کند.
*   پس از آن، هر نشانه (کلمه) در متن را به صورت لماتیزه می کند، که آنها را به فرم های پایه خود تبدیل می کند.
*   در نهایت، با استفاده از Porter Stemmer، stemming را برای هر توکن اعمال می‌کند.
*   سپس توکن‌های ریشه‌دار دوباره به یک رشته متصل می‌شوند و به عنوان متن پیش‌پردازش شده بازگردانده می‌شوند.

به طور خلاصه، ریشه‌یابی در این قطعه کد تکمیل کننده سایر مراحل پیش‌پردازش متن مانند توکن‌سازی، حذف کلید واژه‌ها و واژه‌سازی است تا واژگان را ساده‌تر کرده و داده‌های متنی را برای کارهای پایین‌دستی NLP آماده کند.

## نفاوت lemmatization , stemming
تفاوت اصلی بین lemmatization و stemming در نحوه تقلیل کلمات به شکل پایه یا ریشه آنها نهفته است:

Lemmatization:

Lemmatization کلمات را به شکل پایه یا فرهنگ لغت خود کاهش می دهد که به عنوان لم شناخته می شود.
این شامل تجزیه و تحلیل ساختار صرفی کلمه و در نظر گرفتن زمینه آن برای تعیین لم آن است.
Lemmatization تضمین می کند که فرم پایه به دست آمده یک کلمه معتبر در زبان است.
Example: The lemma of "running", "ran", and "runner" is "run".
stemming:

stemming یک رویکرد ساده تر و اکتشافی تر در مقایسه با واژه سازی است.
این شامل حذف پسوندها یا پیشوندها از کلمات برای به دست آوردن ریشه یا ریشه آنها است.
Stemming بافت کلمه یا اینکه آیا ریشه حاصل کلمه معتبری در زبان است را در نظر نمی گیرد.
ریشه کردن ممکن است منجر به غیرکلمات یا کلماتی شود که شکل اصلی کلمه اصلی نیستند.
Example: The stem of "running", "ran", and "runner" is "runn".
به طور خلاصه:

Lemmatization شکل های پایه دقیق تر و معنی دار تری را در مقایسه با stemming ایجاد می کند.
stemming سریع‌تر و ساده‌تر است، اما ممکن است به شکل‌های پایه کمتر دقیق یا معتبر منجر شود.
زمانی که دقت زبانی مهم است، lemmatization ترجیح داده می شود، مانند کاربردهایی که به تحلیل معنایی یا درک زبان نیاز دارند.
stemming ممکن است برای کارهایی مانند بازیابی اطلاعات یا طبقه‌بندی متن که سرعت بسیار مهم است و دقت زبانی کمتر مهم است، کافی باشد.
"""

# Function to detect language and filter only English articles
def filter_english_articles(text):
    try:
        if detect(text) == 'en':
            return text
        else:
            return ''
    except LangDetectException:
        return ''

# Apply the language filter to the body_text column
df['body_text'] = df['body_text'].apply(filter_english_articles)

# Remove rows with empty body_text after filtering
df = df[df['body_text'] != '']

# Initialize spaCy parser
parser = en_core_sci_lg.load(disable=["tagger", "ner"])
parser.max_length = 3000000

# Stemmer
stemmer = PorterStemmer()

# Function to preprocess text
def preprocess_text(text):
    doc = parser(text.lower())
    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and token.text not in string.punctuation]
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(stemmed_tokens)

# Apply preprocessing to the body_text column
df['processed_body_text'] = df['body_text'].progress_apply(preprocess_text)

# Display the first few rows of the processed dataframe
df.head()

# Save the preprocessed data to a new CSV file
df.to_csv('preprocessed_df.csv', index=False)

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> استخراج ویژگی </b>
<br>
<p align="justify">
<font size=5>
اکنون که متن بدنهٔ مقاله‌ها را از قبل پردازش کرده‌ایم، زمان تبدیل آن‌ها به قالبی است که توسط الگوریتم‌های ما قابل استفاده باشد. برای این منظور از tf-idf استفاده خواهیم کرد. tf_idf  یک الگوریتم بسیار رایج برای تبدیل متن به نمایش معنی دار اعداد است که اهمیت هر کلمه را در متن موردنظر نشان میدهد. <br>
درمورد عملکرد این روش جستجو کنید.
 برای متن پیش پردازش شده هر مقاله، با استفاده از این روش یک بازنمایی برداری با حداکثر 4096 ویژگی ایجاد کنید.
</p>
</font>
</div>
</font>

TF-IDF (Term Frequency-Inverse Document Frequency) یک معیار آماری است که برای ارزیابی اهمیت یک کلمه در یک سند نسبت به مجموعه ای از اسناد (corpus) استفاده می شود.

این چیزی است که در یک مجموعه داده انجام می دهد:

فرکانس مدت (TF):

فراوانی هر عبارت (کلمه) در یک سند را محاسبه می کند.
تعداد دفعاتی که یک عبارت در یک سند ظاهر می شود را اندازه می گیرد.
با استفاده از فرمول محاسبه می‌شود: TF(t,d)=تعداد دفعاتی که عبارت t در سند ظاهر می‌شود را
فرکانس معکوس سند (IDF):

نادر بودن هر عبارت را در تمام اسناد موجود در مجموعه محاسبه می کند.
اهمیت یک اصطلاح را با تعیین منحصر به فرد بودن آن در سراسر مجموعه می سنجد.
با استفاده از فرمول محاسبه می شود: IDF(t,D)=log⁡(تعداد کل اسناد موجود در مجموعه DNumber of document حاوی عبارت t)IDF(t,D)=log(تعداد اسناد حاوی عبارت tTotal تعداد اسناد در پیکره D)
TF-IDF:

امتیازهای TF و IDF را برای محاسبه اهمیت هر عبارت در یک سند در مجموعه ترکیب می کند.
به اصطلاحاتی که اغلب در سند (TF) اما به ندرت در اسناد دیگر (IDF) ظاهر می شوند، وزن بیشتری می دهد.
با ضرب نمرات TF و IDF محاسبه می شود: TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)TF-IDF(t,d,D)=TF(t, د)×IDF(t,D)
در زمینه یک مجموعه داده:

TF-IDF را می توان برای مجموعه ای از اسناد متنی (به عنوان مثال، مقالات، بررسی ها، ایمیل ها) اعمال کرد.
داده های متنی را به بردارهای عددی تبدیل می کند، جایی که هر بعد بیانگر یک عبارت و اهمیت آن است.
با برجسته کردن عبارات مهم در هر سند نسبت به کل مجموعه به کارهایی مانند طبقه بندی اسناد، خوشه بندی و بازیابی اطلاعات کمک می کند.
"""

# Initialize the TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=4096, max_df=0.95, min_df=2, stop_words='english')

# Fit and transform the processed body text
tfidf_matrix = vectorizer.fit_transform(df['processed_body_text'])
feature_names = vectorizer.get_feature_names_out()
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
df_tfidf.head()

# Check the shape of the TF-IDF matrix
print(tfidf_matrix.shape)

"""<font face="B Mitra"><div dir=rtl>
<font size=6>
<b> PCA </b>
<br>
<p align="justify">
<font size=5>
با توجه به ابعاد بالای هر بردار ویژگی، با استفاده از روش PCA با حفظ 95 درصد واریانس کاهش بعد دهید.
</p>
</font>
</div>
"""

# Initialize PCA to retain 95% of the variance
pca = PCA(n_components=0.95, random_state=42)

# Fit and transform the TF-IDF matrix
pca_features = pca.fit_transform(tfidf_matrix.toarray())
df_pca = pd.DataFrame(pca_features)
df_pca.head()

# Check the shape of the reduced feature matrix
print(pca_features.shape)

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> خوشه بندی </b>
<br>
<p align="justify">
<font size=5>
خوشه بندی مقالات تحقیقاتی مشابه با هم می تواند جستجوی انتشارات مرتبط را ساده کند.  در اینجا از روش K-Means برای خوشه بندی استفاده میکنیم.<br>
 با استفاده از روش Elbow Method  از بین تعداد خوشه های داده شده، میتوانید بهترین تعداد خوشه را بیابید.
<br>
تا 30 خوشه این موضوع را مورد بررسی قرار دهید و پس از یافتن تعداد خوشه مناسب، با استفاده از روش K-Means خوشه بندی لازم را برای بردار ویژگی
بدست آماده از مرحلهٔ قبل به کار بگیرید.
<br><b> بخش امتیازی:</b> میتوانید با به کارگیری روش های دیگر خوشه بندی نظیر خوشه بندی سلسه مراتبی یا DBSCAN نتایج را با استفاده از یک معیار ارزیابی مناسب  مقایسه کنید.
</p>
</font>
</div>

# Elbow Method
Elbow Method یک تکنیک اکتشافی است که برای تعیین تعداد بهینه خوشه‌ها در یک مجموعه داده برای الگوریتم‌های خوشه‌بندی مانند K-means استفاده می‌شود. در اینجا نحوه کار آن آمده است:

اعمال الگوریتم خوشه بندی: در ابتدا، الگوریتم خوشه بندی (به عنوان مثال، K-means) را در مجموعه داده برای محدوده ای از مقادیر K (تعداد خوشه ها) اعمال کنید. به طور معمول، K از 1 تا حداکثر تعداد خوشه از پیش تعریف شده متغیر است.

محاسبه مجموع مربعات درون خوشه ای (WCSS): برای هر مقدار K، مجموع مجذورات فواصل بین هر نقطه داده و مرکز آن را محاسبه کنید، که روی همه خوشه ها جمع شده است. این به عنوان مجموع مربع های درون خوشه ای (WCSS) شناخته می شود. فشردگی خوشه ها را اندازه گیری می کند.

رسم منحنی Elbow: مقادیر K را در برابر WCSS مربوطه رسم کنید. این منجر به یک نمودار خطی می شود که در آن محور x تعداد خوشه ها (K) و محور y نشان دهنده WCSS است.

شناسایی نقطه Elbow : منحنی Elbow را بررسی کنید و "نقطه Elbow" را شناسایی کنید، که نقطه ای است که سرعت کاهش WCSS به شدت کاهش می یابد. از نظر بصری، این نقطه نشان دهنده یک خم (یا آرنج) قابل توجه در منحنی است.

انتخاب تعداد بهینه خوشه ها: تعداد بهینه خوشه ها معمولاً به عنوان مقدار K در نقطه آرنج انتخاب می شود. این نقطه ای را نشان می دهد که در آن خوشه های اضافی دیگر به طور قابل توجهی WCSS را کاهش نمی دهند، و افزودن خوشه های بیشتر ممکن است منجر به بیش از حد برازش شود.

تأیید تعداد بهینه خوشه‌ها: توصیه می‌شود تعداد بهینه خوشه‌ها را با استفاده از دانش دامنه، در صورت وجود، یا با ارزیابی عملکرد خوشه‌بندی با استفاده از معیارهای دیگر مانند امتیاز شبح یا انسجام/جداسازی خوشه تأیید کنید.

به طور کلی، روش Elbow یک روش ساده و شهودی برای تخمین تعداد بهینه خوشه‌ها در یک مجموعه داده ارائه می‌کند، اما ممکن است همیشه نتایج واضحی به همراه نداشته باشد، به خصوص اگر مجموعه داده پیچیده باشد یا دارای خوشه‌های همپوشانی باشد. بنابراین، باید به عنوان یک راهنما به جای یک راه حل قطعی استفاده شود.

#DBSCAN
DBSCAN (خوشه‌بندی فضایی برنامه‌های کاربردی با نویز مبتنی بر چگالی) یک الگوریتم خوشه‌بندی است که نقاطی را که به هم نزدیک شده‌اند را گروه‌بندی می‌کند، در حالی که نقاطی را که به تنهایی در مناطق کم چگالی قرار دارند به عنوان نقاط پرت علامت‌گذاری می‌کند. در اینجا نحوه عملکرد DBSCAN آمده است:

خوشه بندی مبتنی بر چگالی:

DBSCAN نقاط را بر اساس چگالی آنها گروه بندی می کند نه اینکه تعداد خاصی از خوشه ها مانند K-means را فرض کنیم.
دو پارامتر مهم را تعریف می کند:
ε (epsilon): شعاعي كه در آن نقاط همسايه جستجو مي شود.
MinPts: حداقل تعداد نقاط مورد نیاز برای تشکیل یک منطقه متراکم.
نقاط اصلی، نقاط مرزی و نویز:

نقاط اصلی: نقاطی با حداقل MinPts در شعاع ε.
نقاط مرزی: نقاطی در شعاع ε از یک نقطه مرکزی، اما MinPts در شعاع ε خودشان ندارند.
Noise Points (Outliers): نقاطی که نه هسته هستند و نه نقاط مرزی.
تشکیل خوشه:

DBSCAN با یک نقطه دلخواه شروع می شود و همه نقاط را در همسایگی ε خود پیدا می کند.
اگر نقطه حداقل همسایه های MinPts داشته باشد، به یک نقطه اصلی تبدیل می شود و همسایگان آن به همان خوشه اضافه می شوند.
DBSCAN به صورت بازگشتی خوشه را با بازدید از همسایگان نقاط اصلی گسترش می دهد.
نقاط مرزی به خوشه نزدیکترین نقطه هسته آنها اختصاص داده می شود.
نقاطی که با هیچ نقطه اصلی قابل دسترسی نیستند، به عنوان نویز یا نقاط پرت برچسب گذاری می شوند.
مزایای:

مقاوم در برابر نویز و نقاط دورافتاده به دلیل توانایی آن در تشخیص مناطق با تراکم های مختلف.
به تعداد از پیش تعریف شده خوشه نیاز ندارد.
می تواند خوشه هایی از اشکال دلخواه را اداره کند.
معایب:

به انتخاب دقیق پارامترهای ε و MinPts نیاز دارد که ممکن است ساده نباشد.
برای داده های با ابعاد بالا کارایی کمتری دارد.
تفاوت بین DBSCAN و K-means:

شکل خوشه:

K-means فرض می کند که خوشه ها کروی و با اندازه مشابه هستند. هر نقطه را به نزدیکترین مرکز تخصیص می دهد، که منجر به خوشه هایی با واریانس مساوی می شود. از سوی دیگر، DBSCAN می تواند خوشه هایی از اشکال و اندازه های دلخواه را شناسایی کند.
تعداد خوشه ها:

K-means از کاربر می خواهد که تعداد خوشه ها (K) را از قبل مشخص کند، در حالی که DBSCAN به طور خودکار تعداد خوشه ها را بر اساس تراکم داده ها تعیین می کند.
رسیدگی به موارد پرت:

K-means نقاط پرت را به عنوان بخشی از نزدیکترین خوشه در نظر می گیرد که ممکن است مرکزهای خوشه را منحرف کند. DBSCAN به صراحت نقاط پرت را به عنوان نقاط نویز شناسایی می کند و آنها را به هیچ خوشه ای اختصاص نمی دهد.
حساسیت پارامتر:

K-means به انتخاب اولیه سانتروئیدها حساس است و بسته به مقدار اولیه ممکن است به راه حل های مختلف همگرا شود. DBSCAN به تنظیمات پارامتر حساسیت کمتری دارد، اگرچه انتخاب ε و MinPts می‌تواند بر نتیجه خوشه‌بندی تأثیر بگذارد.
به طور کلی، DBSCAN برای مجموعه داده‌هایی با اشکال نامنظم و چگالی‌های متفاوت مناسب‌تر است، در حالی که K-means ممکن است برای مجموعه‌های داده با خوشه‌های کروی به خوبی جدا شده ارجح باش
"""

# Use the Elbow Method to find the optimal number of clusters
distortions = []
K = range(1, 31)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_features)
    distortions.append(sum(np.min(cdist(pca_features, kmeans.cluster_centers_, 'euclidean'), axis=1)) / pca_features.shape[0])

# Plot the Elbow Curve
plt.figure(figsize=(10, 6))
plt.plot(K, distortions, 'bx-')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

# Assuming the optimal number of clusters determined from the Elbow method is 10
optimal_k = 10

# Apply KMeans with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(pca_features)

# Assign cluster labels to the articles
df['cluster'] = kmeans.labels_

# Display the first few rows of the dataframe with cluster labels
df.head()

# Apply Agglomerative Clustering
agg_cluster = AgglomerativeClustering(n_clusters=optimal_k)
agg_labels = agg_cluster.fit_predict(pca_features)

# Calculate the silhouette score
agg_silhouette_score = silhouette_score(pca_features, agg_labels)
print(f'Agglomerative Clustering Silhouette Score: {agg_silhouette_score}')

# DBSCAN with parameter tuning
eps_values = np.arange(0.1, 2.0, 0.1)
min_samples_values = range(3, 10)
best_score = -1
best_eps = 0
best_min_samples = 0

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(pca_features)
        if len(set(labels)) > 1 and len(set(labels)) < len(labels):
            score = silhouette_score(pca_features, labels)
            if score > best_score:
                best_score = score
                best_eps = eps
                best_min_samples = min_samples

dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
dbscan_labels = dbscan.fit_predict(pca_features)
print(f'Best DBSCAN eps: {best_eps}, min_samples: {best_min_samples}, Silhouette Score: {best_score}')

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> t-SNE </b>
<br>
<p align="justify">
<font size=5>
t_SNE یک روش نظارت نشده غیر خطی است که برای اکتشاف و بصری‌سازی داده‌ها مورد استفاده قرار می‌گیرد.
PCA یک روش کاهش ابعاد خطی است که در تلاش برای بیشینه کردن واریانس و حفظ فاصله‌های زیاد دوتایی‌ها از یکدیگر است. این امر می‌تواند منجر به بصری‌سازی ضعیف به ویژه هنگام کار با ساختارهای غیرخطی می‌شود. t-SNE  با حفظ فاصله‌های کم دوتایی‌ها یا شباهت محلی از PCA متمایز می‌شود. به بیان ساده‌تر، t-SNE به کاربر درکی از اینکه داده‌ها چگونه در فضای ابعاد بالا سازمان‌دهی شده‌اند را ارائه می‌کند. <br>
با استفاده از t-SNE می توانیم بردار ویژگی هایی با ابعاد بالا را به 2 بعد کاهش دهیم.
در این مجموعه داده در 2 بعد می توان توزیع مقالات را با استفاده از scatterplot نمایش داد. <br>
برای تفکیک بصری موضوعات مختلف در نمودار بالا، با استفاده از خوشه هایی که در KMeans یافتید ، برای رنگ آمیزی خوشه های مختلف در مصورسازی خود میتوانید استفاده کنید.
</p>
</font>
</div>

# t-SNE
t-SNE (t-distributed Stochastic Neighbor Embedding) یک تکنیک کاهش ابعاد است که برای تجسم داده های با ابعاد بالا در فضای با ابعاد پایین تر، معمولاً دو یا سه بعدی استفاده می شود. این به ویژه برای تجسم مجموعه داده های پیچیده با روابط غیر خطی بین متغیرها موثر است. در اینجا نحوه کار آن آمده است:

محاسبه شباهت:

t-SNE با محاسبه شباهت های زوجی بین نقاط داده در فضای با ابعاد بالا شروع می شود.
شباهت بین دو نقطه را با استفاده از تابع هسته گاوسی اندازه گیری می کند که احتمالات بالاتر را به نقاط نزدیک و احتمالات کمتری را به نقاط دور نسبت می دهد.
توزیع احتمال:

t-SNE شباهت های زوجی را به احتمالات شرطی تبدیل می کند که نشان دهنده احتمال انتخاب یک نقطه به عنوان همسایه نقطه دیگر است.
این احتمالات را برای به دست آوردن توزیع احتمال روی همه جفت نقاط عادی می کند.
نقشه برداری با ابعاد کم:

t-SNE توزیع احتمال مشابهی را در فضای کم‌بعدی ایجاد می‌کند (به عنوان مثال، دو یا سه بعدی).
به طور مکرر موقعیت نقاط را در فضای با ابعاد پایین تر تنظیم می کند تا عدم تطابق بین توزیع های احتمالی با ابعاد بالا و ابعاد پایین را به حداقل برساند.
هدف فرآیند بهینه‌سازی حفظ ساختار محلی داده‌ها، حصول اطمینان از این است که نقاط مجاور در فضای با ابعاد بالا در جاسازی با ابعاد پایین‌تر به هم نزدیک می‌مانند.
تابع هزینه:

t-SNE واگرایی Kullback-Leibler (واگرایی KL) را بین توزیع‌های احتمالی با ابعاد بالا و پایین به حداقل می‌رساند.
واگرایی KL تفاوت بین دو توزیع احتمال را اندازه گیری می کند و به عنوان تابع هزینه برای بهینه سازی جاسازی عمل می کند.
تجسم:

هنگامی که فرآیند بهینه سازی همگرا می شود، t-SNE یک جاسازی کم بعدی از داده ها را ایجاد می کند که ساختار محلی و روابط بین نقاط داده را حفظ می کند.
این تعبیه را می توان با استفاده از نمودارهای پراکنده یا سایر تکنیک های تجسم برای به دست آوردن بینش در مورد ساختار زیربنایی داده ها تجسم کرد.
به طور خلاصه، t-SNE یک تکنیک قدرتمند برای تجسم داده های با ابعاد بالا با نگاشت آن به فضایی با ابعاد پایین تر و در عین حال حفظ روابط محلی است. به طور گسترده در تجزیه و تحلیل داده های اکتشافی، تشخیص الگو و خوشه بندی برای به دست آوردن بینش در مورد ساختار مجموعه داده های پیچیده استفاده می شود.
"""

# KMeans clustering
kmeans = KMeans(n_clusters=10, random_state=42, n_init="auto")
kmeans_labels = kmeans.fit_predict(df_pca)

# t-SNE for 2D visualization
tsne = TSNE(n_components=2, random_state=42)
tsne_embeddings = tsne.fit_transform(df_pca)
df['tsne-2d-one'] = tsne_embeddings[:, 0]
df['tsne-2d-two'] = tsne_embeddings[:, 1]
df['y'] = kmeans_labels

# Plot KMeans clusters using t-SNE
plt.figure(figsize=(10, 8))
for i in range(7):
    plt.scatter(tsne_embeddings[kmeans_labels == i, 0],
                tsne_embeddings[kmeans_labels == i, 1],
                label=f'Cluster {i+1}')
plt.title('t-SNE Visualization of K-Means Clustering')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend()
plt.show()

# t-SNE Visualization
def plot_tsne(features, labels, title):
    tsne = TSNE(n_components=2, random_state=42)
    tsne_features = tsne.fit_transform(features)
    tsne_df = pd.DataFrame(tsne_features, columns=['x', 'y'])
    tsne_df['cluster'] = labels
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x='x', y='y', hue='cluster', palette=sns.color_palette('hsv', len(set(labels))), data=tsne_df, legend='full')
    plt.title(title)
    plt.show()

plot_tsne(pca_features, kmeans.labels_, 't-SNE visualization of KMeans clustered articles')
plot_tsne(pca_features, agg_labels, 't-SNE visualization of Agglomerative clustered articles')
if len(set(dbscan_labels)) > 1:
    plot_tsne(pca_features, dbscan_labels, 't-SNE visualization of DBSCAN clustered articles')

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b> مدلسازی موضوعی (Topic Modeling)</b>
<br>
<p align="justify">
<font size=5>
اکنون سعی خواهیم کرد موضوعات کلیدی در هر خوشه را پیدا کنیم.
<br>
 K-means مقالات را خوشه بندی کرد اما موضوعات را برچسب گذاری نکرد. از طریق مدل‌سازی موضوع، متوجه خواهیم شد که مهمترین اصطلاحات برای هر خوشه چیست. این کار با دادن کلمات کلیدی برای شناسایی سریع مضامین خوشه، معنای بیشتری به خوشه می بخشد.<br>
برای مدلسازی موضوعی از الگوریتم های مختلفی نظیر NMF,LSA,LDA میتوان استفاده کرد.
<br>
تعداد موضوعاتی که از خوشه ها قصد دارید استخراج کنید، را به دلخواه انتخاب کنید.
<br>
با استفاده از روش (Latent Dirichlet allocation)LDA موضوعات مناسب را به تعداد موردنظر برای هر خوشه بیابید.
سپس کلمات کلیدی هر موضوع برای خوشه موردنظر را چاپ کنید.
<br><b> بخش امتیازی:</b> مدلسازی موضوعی با روشی غیر از LDA انجام دهید و موضوعات خوشه ها را در هردو روش با هم مقایسه کنید.

</p>
</font>
</div>

#LDA
تخصیص دیریکله پنهان (LDA) یک مدل احتمالی تولیدی است که برای مدل‌سازی موضوع استفاده می‌شود، که تکنیکی برای کشف موضوعات اساسی در مجموعه‌ای از اسناد است. در اینجا نحوه کار LDA آمده است:

مفروضات:

LDA فرض می کند که هر سند در مجموعه ترکیبی از موضوعات مختلف است.
همچنین فرض می‌کند که هر مبحث توزیعی بر واژگان ثابتی از کلمات است.
مقداردهی اولیه:

LDA با اختصاص تصادفی هر کلمه در هر سند به یکی از موضوعات شروع می شود.
فرآیند تکراری:

LDA هر کلمه را در هر سند تکرار می کند و آن را بر اساس دو احتمال به یک موضوع اختصاص می دهد:
احتمال تعلق کلمه به یک موضوع خاص با توجه به تخصیص کلمه فعلی در سند.
احتمال تعلق سند به یک موضوع خاص با توجه به تکالیف موضوع فعلی در مجموعه.
نمونه گیری گیبس:

LDA معمولاً از نمونه‌گیری گیبس، یک روش زنجیره مارکوف مونت کارلو (MCMC)، برای نمونه‌برداری مکرر از توزیع‌های احتمال شرطی تکالیف موضوعی استفاده می‌کند.
نمونه گیری گیبس، تکالیف موضوعی را برای هر کلمه در هر سند بر اساس احتمالات محاسبه شده در مرحله 3 به روز می کند.
همگرایی:

LDA به تکرار از طریق اسناد و کلمات تا زمان همگرایی ادامه می دهد، جایی که تکالیف تثبیت می شوند و موضوعات با احتمال زیاد استنباط می شوند.
خروجی:

پس از همگرایی، خروجی LDA:
موضوعات استنباط شده به صورت توزیع بر روی واژگان کلمات نمایش داده می شوند.
نسبت موضوع برای هر سند، نشان دهنده اهمیت نسبی هر موضوع در سند است.
تفسیر:

محققان می توانند با بررسی محتمل ترین کلمات مرتبط با هر موضوع، موضوعات استنباط شده را تفسیر کنند.
آنها همچنین می توانند نسبت موضوع اسناد را برای درک توزیع موضوعات در مجموعه تجزیه و تحلیل کنند.
به طور خلاصه، LDA یک مدل احتمالی است که اسناد را به عنوان مخلوطی از موضوعات و موضوعات را به عنوان توزیع بر روی کلمات نشان می دهد. با به‌روزرسانی مکرر تکالیف موضوعی با استفاده از نمونه‌گیری گیبس، LDA موضوعات پنهان موجود در مجموعه‌ای از اسناد را آشکار می‌کند و تحلیل اکتشافی و وظایف بازیابی اطلاعات را تسهیل می‌کند.

# NMF
فاکتورسازی ماتریس غیر منفی (NMF) یک تکنیک کاهش ابعاد است که به طور گسترده برای
مدل‌سازی موضوع، به‌ویژه در زمینه داده‌های متنی استفاده می‌شود. در اینجا نحوه عملکرد NMF برای مدل‌سازی موضوع آمده است:

فاکتورسازی ماتریس غیر منفی:

NMF یک ماتریس غیر منفی (به عنوان مثال، یک ماتریس سند-ترم که نشان دهنده بسامدهای کلمه در اسناد است) را به دو ماتریس غیر منفی با ابعاد پایین تر تجزیه می کند: ماتریس سند-موضوع و ماتریس موضوع-ترم.
ماتریس سند-موضوع نشان دهنده توزیع موضوعات در هر سند است.
ماتریس موضوع-ترم نشان دهنده توزیع اصطلاحات (کلمات) در هر موضوع است.
مقداردهی اولیه:

NMF با مقادیر اولیه تصادفی برای ماتریس های سند-موضوع و موضوع-ترم شروع می شود.
بهینه سازی تکراری:

NMF به طور مکرر ماتریس های سند-موضوع و موضوع-ترم را به روز می کند تا خطای بازسازی بین ماتریس اصلی و فاکتورسازی آن را به حداقل برساند.
معمولاً از تکنیک های بهینه سازی مانند نزول گرادیان یا قوانین به روز رسانی ضربی برای به روز رسانی ماتریس ها استفاده می کند.
محدودیت غیر منفی:

NMF محدودیت های غیر منفی را در ماتریس های سند-موضوع و موضوع-ترم اعمال می کند، به این معنی که همه عناصر این ماتریس ها باید غیر منفی باشند.
این محدودیت تضمین می‌کند که موضوعات حاصل و توزیع‌های موضوع سند قابل تفسیر و معنادار هستند.
محدودیت پراکندگی:

به صورت اختیاری، NMF همچنین می‌تواند محدودیت‌های پراکندگی را بر روی ماتریس‌های سند-موضوع و موضوع-ترم اعمال کند تا نمایش‌های پراکنده را تشویق کند.
Sparity کشف موضوعات متمایز و قابل تفسیر را با اختصاص وزن زیاد به تعداد کمی از اصطلاحات برای هر موضوع و تعداد کمی از موضوعات برای هر سند ترویج می کند.
معیارهای بهینه سازی:

معیارهای بهینه‌سازی برای NMF معمولاً شامل به حداقل رساندن هنجار فروبنیوس یا واگرایی Kullback-Leibler بین ماتریس اصلی و فاکتورسازی آن است.
خروجی:

پس از همگرایی، NMF ماتریس های سند-موضوع و موضوع-ترم را تولید می کند که ساختار موضوع استنباط شده مجموعه اسناد را نشان می دهد.
محققان می توانند با بررسی مهم ترین اصطلاحات مرتبط با هر موضوع در ماتریس موضوع-ترم، موضوعات را تفسیر کنند.
به طور خلاصه، NMF یک تکنیک قدرتمند برای مدل‌سازی موضوع است که یک ماتریس سند-ترم را به موضوعات قابل تفسیر و توزیع آن‌ها بر روی شرایط و اسناد تجزیه می‌کند. NMF با اعمال محدودیت‌های غیر منفی و اختیاری پراکندگی، کشف موضوعات معنادار و منسجم را در داده‌های متنی تسهیل می‌کند.

# مقایسه ی LDA , NMF
هر دو تخصیص دیریکله پنهان (LDA) و فاکتورسازی ماتریس غیر منفی (NMF) الگوریتم های محبوبی برای مدل سازی موضوع هستند، اما آنها در اصول اساسی، مفروضات و رویکردهای محاسباتی خود تفاوت هایی دارند. در اینجا مقایسه بین LDA و NMF وجود دارد:

فاکتورسازی مولد در مقابل ماتریس:

LDA یک مدل احتمالی تولیدی است که فرض می‌کند هر سند ترکیبی از موضوعات است و هر کلمه در سند توسط یکی از موضوعات ایجاد می‌شود. این فرآیند تولید اسناد را مدل می کند.
NMF یک تکنیک فاکتورسازی ماتریسی است که یک ماتریس سند-ترم را به دو ماتریس با ابعاد پایین تر تجزیه می کند که توزیع های سند-موضوع و موضوع-ترم را نشان می دهد. مستقیماً ماتریس داده‌های مشاهده‌شده را بدون مدل‌سازی صریح فرآیند تولیدی فاکتورسازی می‌کند.
محدودیت های غیر منفی:

NMF محدودیت‌های غیر منفی را در ماتریس‌های سند-موضوع و موضوع-ترم اعمال می‌کند، به این معنی که همه عناصر این ماتریس‌ها باید غیرمنفی باشند. این تضمین می‌کند که موضوعات حاصل و توزیع‌های سند-موضوع قابل تفسیر و معنادار هستند.
LDA دارای محدودیت‌های غیر منفی داخلی نیست، اما به طور ضمنی توزیع‌های غیرمنفی سند-موضوع و موضوع-اصطلاح را از طریق توزیع دیریکله ایجاد می‌کند.
قابلیت تفسیر:

هر دو LDA و NMF توزیع‌های موضوعی را تولید می‌کنند که می‌تواند برای درک موضوعات اساسی در مجموعه اسناد تفسیر شود.
NMF ممکن است به دلیل فاکتورسازی مستقیم ماتریس داده های مشاهده شده و محدودیت های غیر منفی اجباری قابل تفسیرتر باشد.
موضوعات LDA توزیع در کل واژگان هستند، در حالی که موضوعات NMF توزیع بر روی اصطلاحاتی هستند که مستقیماً با اصطلاحات مشاهده شده در مجموعه اسناد مطابقت دارند.
پراکندگی:

NMF می‌تواند محدودیت‌های پراکندگی را برای تشویق بازنمایی‌های پراکنده موضوعات و اسناد بکار گیرد و کشف موضوعات متمایز و قابل تفسیر را ترویج کند.
LDA به صراحت محدودیت های پراکندگی را در بر نمی گیرد، اما پراکندگی را می توان از طریق فراپارامترها یا تکنیک های پس پردازش القا کرد.
پیچیدگی محاسباتی:

LDA معمولاً شامل تکنیک‌های استنتاج مبتنی بر نمونه‌برداری مانند نمونه‌گیری گیبس یا استنتاج متغیر است که می‌تواند از نظر محاسباتی گران باشد، به ویژه برای مجموعه داده‌های بزرگ.
NMF معمولاً شامل رویکردهای مبتنی بر بهینه‌سازی مانند نزول گرادیان یا به‌روزرسانی‌های ضربی است که عموماً سریع‌تر و مقیاس‌پذیرتر هستند.
به طور خلاصه، LDA و NMF هر دو الگوریتم‌های موثری برای مدل‌سازی موضوع هستند که هر کدام نقاط قوت و ضعف خود را دارند. LDA یک مدل احتمالی مولد است که به صراحت فرآیند تولید اسناد را مدل می کند، در حالی که NMF یک تکنیک فاکتورسازی ماتریسی است که مستقیماً ماتریس داده های مشاهده شده را فاکتورسازی می کند. انتخاب بین LDA و NMF به عواملی مانند قابلیت تفسیر، پیچیدگی محاسباتی و ویژگی های خاص مجموعه داده بستگی دارد.
"""

# Function to print the top words for each topic
def print_top_words(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx + 1}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))
    print()

# Number of topics
n_topics = optimal_k

# Perform LDA
lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online', random_state=42)
lda.fit(tfidf_matrix)
print("\nTopics in LDA model:")
tfidf_feature_names = vectorizer.get_feature_names_out()
print_top_words(lda, tfidf_feature_names, 10)

# Perform NMF
nmf = NMF(n_components=n_topics, random_state=42, init='nndsvd')
nmf.fit(tfidf_matrix)
print("\nTopics in NMF model:")
print_top_words(nmf, tfidf_feature_names, 10)

# Display top words in each cluster using LDA
print("\nTop words in each cluster using LDA:")
for cluster_idx in range(optimal_k):
    cluster_documents = df[df['cluster'] == cluster_idx]['processed_body_text']
    cluster_tfidf_matrix = vectorizer.transform(cluster_documents)
    lda_cluster = LatentDirichletAllocation(n_components=n_topics, max_iter=10, learning_method='online', random_state=42)
    lda_cluster.fit(cluster_tfidf_matrix)
    print(f"Cluster {cluster_idx + 1}:")
    print_top_words(lda_cluster, tfidf_feature_names, 10)

# Visualization
def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(1, n_topics, figsize=(30, 15), sharex=True)
    axes = axes.flatten()
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_ind]
        weights = topic[top_features_ind]
        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.7)
        ax.set_title(f'Topic {topic_idx + 1}', fontdict={'fontsize': 10})
        ax.invert_yaxis()
        ax.tick_params(axis='both', which='major', labelsize=10)
        for i in 'top right left'.split():
            ax.spines[i].set_visible(False)
    fig.suptitle(title, fontsize=12)
    plt.subplots_adjust(top=0.9, bottom=0.05, wspace=0.90, hspace=0.3)
    plt.show()

plot_top_words(lda, tfidf_feature_names, 10, 'Top words in LDA topics')
plot_top_words(nmf, tfidf_feature_names, 10, 'Top words in NMF topics')

"""<font face="B Mitra">
<div dir=rtl>
<font size=6>
<b>  مصورسازی </b>
<br>
<p align="justify">
<font size=4>
<b>این بخش شامل نمره نیست و صرفا جهت درک شما از مراحل انجام شده است</b><br>
<font size=5>
مراحل قبلی برچسب‌های خوشه‌بندی و مجموعه‌ای از مقالات را به دو بعد کاهش داده است. می‌توانیم یک طرح تعاملی از خوشه ها ایجاد کنیم. <br>
شما میتوانید با انتخاب هر خوشه از اسلایدر، توزیع هر خوشه را به صورت مجزا مشاهده کنید. همچنین میتوانید با بردن نشانگر روی هریک از نقاط مشخصات کلی آن مقاله را مشاهده کنید.
<br>
<img src="https://drive.google.com/uc?id=14xXSuD-FhmSSJBI0oH-a-DDkgjBtqj4_" alt="Linear Algebra Cover Art" width="800">
</p>
</font>
</div>

<font face="B Mitra">
<div dir=rtl>
<p align="justify">
<font size=5>
ورودی تابع زیر، دیتافریم مقالات می‌باشد که می‌بایست شامل ستون‌های زیر باشد:
<br>
1. <code>tsne-2d-one</code>: مولفهٔ اول t-SNE
<br>
2. <code>tsne-2d-two</code>: مولفهٔ دوم t-SNE
<br>
3. <code>y</code>: خوشهٔ هر یک از مقالات
</p>
</font>
</div>
"""

import plotly.express as px
import plotly.graph_objects as go

# Assuming df contains the t-SNE reduced features and cluster labels
df['tsne-2d-one'] = tsne_embeddings[:, 0]
df['tsne-2d-two'] = tsne_embeddings[:, 1]
df['y'] = kmeans.labels_  # Assuming KMeans labels, adjust as needed

def interactive_plot(df):
    clusters = sorted(df['y'].unique())

    # Generate distinct colors for each cluster using Plotly's colors
    colors = px.colors.qualitative.Plotly

    # Create a scatter plot for each cluster with a unique color
    data = []
    for i, cluster in enumerate(clusters):
        cluster_data = df[df['y'] == cluster]
        scatter = go.Scatter(
            x=cluster_data['tsne-2d-one'],
            y=cluster_data['tsne-2d-two'],
            mode='markers',
            name=f'Cluster {cluster}',
            text=cluster_data['title'],
            hoverinfo='text',
            marker=dict(size=10, color=colors[i % len(colors)]),  # Assign a unique color
            visible=False  # Initially, make all traces invisible
        )
        data.append(scatter)

    # Add a scatter plot for all clusters with different colors
    scatter_all = go.Scatter(
        x=df['tsne-2d-one'],
        y=df['tsne-2d-two'],
        mode='markers',
        name='All Clusters',
        text=df['title'],
        hoverinfo='text',
        marker=dict(size=10, color=df['y'].map(lambda x: colors[x % len(colors)])),  # Assign colors by cluster
        visible=True  # Initially, show all clusters
    )
    data.append(scatter_all)

    x_min = df['tsne-2d-one'].min()
    x_max = df['tsne-2d-one'].max()
    y_min = df['tsne-2d-two'].min()
    y_max = df['tsne-2d-two'].max()

    # Initialize the figure
    fig = go.Figure(data=data)

    # Ensure square aspect ratio
    fig.update_layout(
        title='Cluster Visualization',
        xaxis=dict(title='t-SNE 1', range=[x_min, x_max]),  # Ensure x and y axes are equal
        yaxis=dict(title='t-SNE 2', range=[y_min, y_max]),
        width=1000,
        height=700  # Ensure the figure is square-shaped
    )

    # Add hover functionality
    fig.update_traces(
        hoverinfo='text',
        marker=dict(opacity=0.7, size=8),
    )

    # Add interactive slider for cluster selection
    steps = []
    for i, cluster in enumerate(clusters):
        step = dict(
            method='update',
            args=[{'visible': [False] * len(clusters) + [False]},  # Hide all clusters
                {'title': f'Cluster {cluster}'}],
            label=f'Cluster {cluster}'
        )
        # Only make the current cluster visible
        step['args'][0]['visible'][i] = True
        steps.append(step)

    # Add final step for all clusters
    steps.append(dict(
        method='update',
        args=[{'visible': [False] * len(clusters) + [True]},  # Only show the 'all clusters' trace
            {'title': 'All Clusters'}],
        label='All Clusters'
    ))

    sliders = [dict(
        active=len(steps) - 1,
        currentvalue={"prefix": "Cluster: "},
        pad={"t": 50},
        steps=steps
    )]

    fig.update_layout(
        sliders=sliders
    )

    # Display the plot
    fig.show()

# Run the interactive plot
interactive_plot(df)